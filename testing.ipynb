{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "48b9e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "# !pip install -q  gradio\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import face_recognition\n",
    "from PIL import Image, ImageDraw\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.widgets import *\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gradio as gr\n",
    "from rembg import remove\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62f80f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exists: True\n",
      "Feature vectors exist: True\n",
      "Profile images exist: True\n"
     ]
    }
   ],
   "source": [
    "model_path = Path('./42amman_face_model.pkl')\n",
    "features_path = Path('./42amman_face_features.npy')\n",
    "profiles_path = Path('./amman_profile_images')\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"Model exists: {model_path.exists()}\")\n",
    "print(f\"Feature vectors exist: {features_path.exists()}\")\n",
    "print(f\"Profile images exist: {profiles_path.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b905b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Loaded feature vectors for 767 students\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "learn = load_learner(model_path)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Load feature vectors\n",
    "feature_vectors = np.load(features_path, allow_pickle=True).item()\n",
    "print(f\"Loaded feature vectors for {len(feature_vectors)} students\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc1c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing recognition with image: sata.jpg\n",
      "Using device: mps\n",
      "Query feature shape: (512, 7, 7)\n",
      "Database feature shape: (512, 7, 7)\n",
      "Flattened shapes - Query: (25088,), DB: (25088,)\n",
      "No matches found above the similarity threshold\n"
     ]
    }
   ],
   "source": [
    "def process_image(img_path, output_path):\n",
    "    \"\"\"Detect face in image, crop, and save to output path\"\"\"\n",
    "    try:\n",
    "        # Load image using face_recognition\n",
    "        if isinstance(img_path, str) or isinstance(img_path, Path):\n",
    "            image = face_recognition.load_image_file(img_path)\n",
    "        else:  # Assume it's a numpy array\n",
    "            image = img_path\n",
    "\n",
    "        # Find all face locations in the image\n",
    "        face_locations = face_recognition.face_locations(image, model=\"cnn\")\n",
    "\n",
    "        if not face_locations:\n",
    "            print(f\"No face found in the image\")\n",
    "            return False\n",
    "\n",
    "        # For simplicity, we'll use the first face found\n",
    "        top, right, bottom, left = face_locations[0]\n",
    "\n",
    "        # Add some margin to the face crop (20% of face size)\n",
    "        height, width = bottom - top, right - left\n",
    "        margin_h, margin_w = int(height * 0.2), int(width * 0.2)\n",
    "\n",
    "        # Adjust boundaries with margins and ensure they're within image bounds\n",
    "        img_h, img_w = image.shape[:2]\n",
    "        top = max(0, top - margin_h)\n",
    "        bottom = min(img_h, bottom + margin_h)\n",
    "        left = max(0, left - margin_w)\n",
    "        right = min(img_w, right + margin_w)\n",
    "\n",
    "        # Crop the image to focus on the face\n",
    "        face_image = image[top:bottom, left:right]\n",
    "        pil_image = Image.fromarray(face_image)\n",
    "\n",
    "        # Resize to a standard size (224x224 for resnet34)\n",
    "        pil_image = pil_image.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "\n",
    "        # Save the processed image\n",
    "        pil_image.save(output_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_features(learn, img_path):\n",
    "    \"\"\"Extract features from the penultimate layer of the model\"\"\"\n",
    "    import torch\n",
    "\n",
    "    # Load and transform the image\n",
    "    img = PILImage.create(img_path)\n",
    "\n",
    "    # Create a test batch with a single image\n",
    "    batch = learn.dls.test_dl([img])\n",
    "\n",
    "    # Determine the device and move data appropriately\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Get the model's feature extractor (everything except the final layer)\n",
    "    feature_extractor = learn.model[:-1].to(device)\n",
    "\n",
    "    # Ensure the model is in eval mode\n",
    "    feature_extractor.eval()\n",
    "\n",
    "    # Extract features with gradient calculation disabled for efficiency\n",
    "    with torch.no_grad():\n",
    "        # Get the first batch from the dataloader\n",
    "        batch_data = first(batch)\n",
    "\n",
    "        # Handle different batch data structures\n",
    "        if isinstance(batch_data, tuple):\n",
    "            # Always take the first element from the tuple, which should be the input tensor\n",
    "            x = batch_data[0]\n",
    "        else:\n",
    "            # It's already a tensor\n",
    "            x = batch_data\n",
    "\n",
    "        # Move tensor to device\n",
    "        x = x.to(device)\n",
    "\n",
    "        # Get the embeddings/activations from the feature extractor\n",
    "        activations = feature_extractor(x)\n",
    "\n",
    "    # Convert to numpy array (first taking it back to CPU if needed)\n",
    "    return activations[0].cpu().numpy()\n",
    "\n",
    "def recognize_face(img_path, threshold=0.7):\n",
    "    \"\"\"Recognize a face in the given image\n",
    "\n",
    "    Args:\n",
    "        img_path: Path to the query image\n",
    "        threshold: Cosine similarity threshold (higher = more strict)\n",
    "\n",
    "    Returns:\n",
    "        List of potential matches with similarity scores\n",
    "    \"\"\"\n",
    "    # Create working directory if it doesn't exist\n",
    "    work_dir = Path('./')\n",
    "    work_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Process the query image to extract face\n",
    "    query_face_path = work_dir/'query_face.jpg'\n",
    "    face_found = process_image(img_path, query_face_path)\n",
    "\n",
    "    if not face_found:\n",
    "        return \"No face detected in the query image\", None\n",
    "\n",
    "    # Extract features from the query face\n",
    "    query_features = extract_features(learn, query_face_path)\n",
    "\n",
    "    # Debug information\n",
    "    print(f\"Query feature shape: {query_features.shape}\")\n",
    "\n",
    "    # Calculate cosine similarity with all known faces\n",
    "    similarities = {}\n",
    "    for name, features in feature_vectors.items():\n",
    "        # Ensure features are flattened to 1D if needed\n",
    "        flat_query = query_features.flatten()\n",
    "        flat_features = np.array(features).flatten()\n",
    "\n",
    "        # Print shape information for first item to debug\n",
    "        if len(similarities) == 0:\n",
    "            print(f\"Database feature shape: {np.array(features).shape}\")\n",
    "            print(f\"Flattened shapes - Query: {flat_query.shape}, DB: {flat_features.shape}\")\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarity = np.dot(flat_query, flat_features) / (\n",
    "            np.linalg.norm(flat_query) * np.linalg.norm(flat_features))\n",
    "\n",
    "        # Store as single float value\n",
    "        similarities[name] = float(similarity)\n",
    "\n",
    "    # Sort by similarity score (highest first)\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Filter by threshold\n",
    "    matches = [match for match in sorted_similarities if match[1] >= threshold]\n",
    "\n",
    "    if not matches:\n",
    "        return \"No matches found above the similarity threshold\", sorted_similarities[:5]\n",
    "\n",
    "    return matches, query_face_path\n",
    "\n",
    "# Test on a profile image (should match perfectly)\n",
    "test_img = list(profiles_path.glob('*.jpg'))[10]\n",
    "print(f\"Testing recognition with image: {test_img.name}\")\n",
    "\n",
    "matches, query_face_path = recognize_face(test_img, threshold=0.8)\n",
    "\n",
    "if isinstance(matches, str):\n",
    "    print(matches)\n",
    "else:\n",
    "    print(f\"Found {len(matches)} potential matches:\")\n",
    "    for name, score in matches[:5]:  # Show top 5 matches\n",
    "        print(f\"  - {name}: {score:.4f}\")\n",
    "\n",
    "def visualize_matches(query_face_path, matches, top_n=5):\n",
    "    \"\"\"Visualize the query face and top matches\"\"\"\n",
    "    if isinstance(matches, str):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.text(0.5, 0.5, matches, fontsize=14, ha='center')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    # Get top N matches\n",
    "    top_matches = matches[:min(top_n, len(matches))]\n",
    "\n",
    "    # Create figure with query face and top matches\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Display query face\n",
    "    ax = fig.add_subplot(1, len(top_matches) + 1, 1)\n",
    "    query_img = plt.imread(query_face_path)\n",
    "    ax.imshow(query_img)\n",
    "    ax.set_title(\"Query Face\", fontsize=14)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Display top matches\n",
    "    for i, (name, score) in enumerate(top_matches):\n",
    "        # Get the original profile image for this match\n",
    "        match_img_path = profiles_path/f\"{name}.jpg\"\n",
    "\n",
    "        if not match_img_path.exists():\n",
    "            continue\n",
    "\n",
    "        ax = fig.add_subplot(1, len(top_matches) + 1, i + 2)\n",
    "        match_img = plt.imread(match_img_path)\n",
    "        ax.imshow(match_img)\n",
    "        ax.set_title(f\"{name}\\nScore: {score:.4f}\", fontsize=12)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the test results\n",
    "if not isinstance(matches, str) and query_face_path is not None:\n",
    "    visualize_matches(query_face_path, matches)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d0499c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://24035e623a3936fd4f.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://24035e623a3936fd4f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recognize_face_gradioe(image, threshold=0.7):\n",
    "    \"\"\"Recognize face for Gradio interface\"\"\"\n",
    "    if image is None:\n",
    "        return \"Please upload an image\"\n",
    "\n",
    "    # Save the uploaded image temporarily\n",
    "    temp_path = './image.png'\n",
    "    Image.fromarray(image).save(temp_path)\n",
    "\n",
    "    # Recognize the face\n",
    "    matches, query_face_path = recognize_face(temp_path, threshold=threshold)\n",
    "\n",
    "    if isinstance(matches, str):\n",
    "        return matches\n",
    "\n",
    "    # Format the results\n",
    "    result = \"\"\n",
    "    if len(matches) > 0:\n",
    "        result += f\"Found {len(matches)} potential matches:\\n\\n\"\n",
    "        for i, (name, score) in enumerate(matches[:5]):  # Show top 5 matches\n",
    "            result += f\"{i+1}. {name}: Confidence {score:.2%}\\n\"\n",
    "    else:\n",
    "        result = \"No matches found above the threshold.\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=recognize_face_gradioe,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"numpy\", label=\"Upload a face image\"),\n",
    "        gr.Slider(minimum=0.5, maximum=0.95, value=0.7, step=0.05, label=\"Similarity Threshold\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Recognition Results\"),\n",
    "    title=\"42 Amman Face Recognition\",\n",
    "    description=\"Upload a photo to check if it matches a student from 42 Amman.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "290c9113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create processed data directory\n",
    "PROCESSED_PATH = Path('./processed_faces')\n",
    "PROCESSED_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "def process_image(img_path, output_path):\n",
    "    \"\"\"Detect face in image, crop, and save to output path\"\"\"\n",
    "    try:\n",
    "        # Load image using face_recognition\n",
    "        image = face_recognition.load_image_file(img_path)\n",
    "\n",
    "        # Find all face locations in the image\n",
    "        face_locations = face_recognition.face_locations(image, model=\"hog\")\n",
    "\n",
    "        if not face_locations:\n",
    "            print(f\"No face found in {img_path}\")\n",
    "            return False\n",
    "\n",
    "        # For simplicity, we'll use the first face found\n",
    "        top, right, bottom, left = face_locations[0]\n",
    "\n",
    "        # Add some margin to the face crop (20% of face size)\n",
    "        height, width = bottom - top, right - left\n",
    "        margin_h, margin_w = int(height * 0.2), int(width * 0.2)\n",
    "\n",
    "        # Adjust boundaries with margins and ensure they're within image bounds\n",
    "        img_h, img_w = image.shape[:2]\n",
    "        top = max(0, top - margin_h)\n",
    "        bottom = min(img_h, bottom + margin_h)\n",
    "        left = max(0, left - margin_w)\n",
    "        right = min(img_w, right + margin_w)\n",
    "\n",
    "        # Crop the image to focus on the face\n",
    "        face_image = image[top:bottom, left:right]\n",
    "        pil_image = Image.fromarray(face_image)\n",
    "\n",
    "        # Resize to a standard size (224x224 for resnet34)\n",
    "        pil_image = pil_image.resize((224, 224), Image.Resampling.LANCZOS)\n",
    "\n",
    "        # Save the processed image\n",
    "        pil_image.save(output_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "        return False\n",
    "    \n",
    "process_image(\"yaltayeh_slack.jpg\", \"test.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eb41e2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed image to test.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "        \n",
    "def remove_bg(img_tensor, output_path, mean=0.5, std=0.5):\n",
    "    \"\"\"\n",
    "    Denormalizes and saves a grayscale image tensor to a file.\n",
    "    \"\"\"\n",
    "    img = img_tensor.clone().detach().cpu()\n",
    "\n",
    "    img = img * std + mean  # Denormalize\n",
    "    img = img.clamp(0, 1)\n",
    "    img_np = img.squeeze().numpy()  # shape: [H, W]\n",
    "    img_uint8 = (img_np * 255).astype(np.uint8)\n",
    "\n",
    "    img = Image.fromarray(img_uint8, mode='L')\n",
    "\n",
    "    face_only = remove(img)\n",
    "\n",
    "    return face_only\n",
    "\n",
    "def process_image(img_path, output_path):\n",
    "    \"\"\"\n",
    "    Detects face in an image, crops it, converts to grayscale, normalizes,\n",
    "    and saves the result to output_path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image using face_recognition\n",
    "        image = face_recognition.load_image_file(img_path, \"L\")\n",
    "\n",
    "        # Detect face\n",
    "        face_locations = face_recognition.face_locations(image, model=\"hog\")\n",
    "        if not face_locations:\n",
    "            print(f\"No face found in {img_path}\")\n",
    "            return False\n",
    "\n",
    "        # Use first face found\n",
    "        top, right, bottom, left = face_locations[0]\n",
    "        height, width = bottom - top, right - left\n",
    "        margin_h, margin_w = int(height * 0.2), int(width * 0.2)\n",
    "        img_h, img_w = image.shape[:2]\n",
    "        top = max(0, top - margin_h)\n",
    "        bottom = min(img_h, bottom + margin_h)\n",
    "        left = max(0, left - margin_w)\n",
    "        right = min(img_w, right + margin_w)\n",
    "\n",
    "        # Crop and resize\n",
    "        face_image = image[top:bottom, left:right]\n",
    "        pil_image = Image.fromarray(face_image).resize((224, 224), Image.Resampling.LANCZOS)\n",
    "\n",
    "        # Convert to grayscale\n",
    "\n",
    "        # Normalize with torchvision transforms\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Converts to [1, H, W] and scales to [0,1]\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "        ])\n",
    "        img_tensor = transform(pil_image)\n",
    "        \n",
    "        # Save image (denormalized for viewing)\n",
    "        face_only = remove_bg(img_tensor, output_path)\n",
    "\n",
    "        face_only.convert(\"L\").save(output_path)\n",
    "        print(f\"Saved processed image to {output_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "process_image(\"yaltayeh_slack.jpg\", \"test.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Image.open(\"test.jpg\")\n",
    "output_image =   remove(input_image)# Removes background\n",
    "output_image.save(\"face_only.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7ff2364d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b21a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
