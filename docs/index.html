<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Recognition System Documentation</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
        }
        header {
            background: linear-gradient(135deg, #00a1e0 0%, #0078d4 100%);
            color: white;
            padding: 30px 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            text-align: center;
        }
        h1 {
            margin: 0;
            font-size: 2.5em;
        }
        h2 {
            color: #0078d4;
            border-bottom: 2px solid #0078d4;
            padding-bottom: 5px;
            margin-top: 40px;
        }
        h3 {
            color: #333;
            margin-top: 25px;
        }
        pre {
            background-color: #f5f5f5;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            border-left: 4px solid #0078d4;
        }
        code {
            font-family: 'Courier New', Courier, monospace;
            background-color: #f5f5f5;
            padding: 2px 4px;
            border-radius: 3px;
        }
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            margin-bottom: 30px;
        }
        .workflow {
            display: flex;
            flex-direction: column;
            align-items: center;
            margin: 30px 0;
        }
        .workflow-step {
            display: flex;
            align-items: center;
            margin-bottom: 15px;
            width: 100%;
        }
        .step-number {
            background-color: #0078d4;
            color: white;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            display: flex;
            justify-content: center;
            align-items: center;
            font-weight: bold;
            margin-right: 15px;
            flex-shrink: 0;
        }
        .step-content {
            flex-grow: 1;
            padding: 15px;
            background-color: #f0f7ff;
            border-radius: 6px;
            border-left: 4px solid #0078d4;
        }
        .image-comparison {
            display: flex;
            justify-content: space-around;
            flex-wrap: wrap;
            margin: 20px 0;
        }
        .image-container {
            margin: 10px;
            text-align: center;
        }
        .image-placeholder {
            width: 224px;
            height: 224px;
            background-color: #e9ecef;
            display: flex;
            justify-content: center;
            align-items: center;
            border-radius: 5px;
            margin-bottom: 10px;
        }
        .arrow {
            font-size: 2em;
            display: flex;
            align-items: center;
            margin: 0 10px;
            color: #0078d4;
        }
        .note {
            background-color: #fff8e6;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }
        .highlighter {
            background-color: #e6f7ff;
            padding: 2px 4px;
            border-radius: 3px;
        }
        .diagram {
            width: 100%;
            max-width: 800px;
            margin: 20px auto;
            padding: 10px;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }
        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        .card {
            background-color: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
        }
        .siamese-network {
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        footer {
            text-align: center;
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #ddd;
            color: #666;
        }
    </style>
</head>
<body>
    <header>
        <h1>42 Amman Face Recognition System</h1>
        <p>A comprehensive guide to the face recognition system built with FastAI and ResNet34</p>
    </header>

    <div class="container">
        <h2>Project Overview</h2>
        <p>
            This documentation explains the 42 Amman Face Recognition System, a deep learning-based solution that can identify whether an uploaded photo matches a student from 42 Amman based on their profile picture. The system uses state-of-the-art face detection, feature extraction, and similarity matching techniques.
        </p>

        <div class="workflow">
            <div class="workflow-step">
                <div class="step-number">1</div>
                <div class="step-content">
                    <strong>Data Collection and Preparation</strong>: Process profile pictures to extract and align faces
                </div>
            </div>
            <div class="workflow-step">
                <div class="step-number">2</div>
                <div class="step-content">
                    <strong>Dataset Creation</strong>: Generate pairs of faces (same/different students) for training
                </div>
            </div>
            <div class="workflow-step">
                <div class="step-number">3</div>
                <div class="step-content">
                    <strong>Model Training</strong>: Train a ResNet34 model to distinguish between same and different faces
                </div>
            </div>
            <div class="workflow-step">
                <div class="step-number">4</div>
                <div class="step-content">
                    <strong>Feature Extraction</strong>: Extract face embeddings from the trained model
                </div>
            </div>
            <div class="workflow-step">
                <div class="step-number">5</div>
                <div class="step-content">
                    <strong>Recognition System</strong>: Compare new faces against known profiles using cosine similarity
                </div>
            </div>
        </div>
    </div>

    <div class="container">
        <h2>System Requirements</h2>
        <p>The face recognition system requires the following dependencies:</p>
        
        <div class="grid">
            <div class="card">
                <h3>Core Libraries</h3>
                <ul>
                    <li>Python 3.8+</li>
                    <li>NumPy</li>
                    <li>Pandas</li>
                    <li>PIL (Pillow)</li>
                    <li>Matplotlib</li>
                </ul>
            </div>

            <div class="card">
                <h3>Deep Learning</h3>
                <ul>
                    <li>FastAI 2.7.12</li>
                    <li>PyTorch</li>
                    <li>face_recognition</li>
                    <li>dlib</li>
                </ul>
            </div>

            <div class="card">
                <h3>Data Augmentation</h3>
                <ul>
                    <li>Albumentations</li>
                </ul>
            </div>

            <div class="card">
                <h3>Interface (Optional)</h3>
                <ul>
                    <li>Gradio (for interactive UI)</li>
                </ul>
            </div>
        </div>

        <pre>pip install fastai==2.7.12 face-recognition dlib albumentations numpy pandas pillow matplotlib gradio</pre>
    </div>

    <div class="container">
        <h2>Step 1: Data Preparation and Face Detection</h2>
        
        <p>
            The first step in building the face recognition system is to prepare the dataset by detecting and cropping faces from profile pictures. This ensures that the model focuses on facial features rather than backgrounds or other irrelevant details.
        </p>

        <h3>Face Detection Process</h3>

        <div class="image-comparison">
            <div class="image-container">
                <div class="image-placeholder">Original Profile Picture</div>
                <p>Raw student profile image</p>
            </div>
            <div class="arrow">→</div>
            <div class="image-container">
                <div class="image-placeholder">Face Detection</div>
                <p>HOG-based face detection</p>
            </div>
            <div class="arrow">→</div>
            <div class="image-container">
                <div class="image-placeholder">Cropped & Aligned Face</div>
                <p>Standardized 224×224 face image</p>
            </div>
        </div>

        <h3>Key Processing Steps:</h3>
        <ol>
            <li><strong>Face Detection</strong>: Using the face_recognition library with HOG-based detection to locate faces in images</li>
            <li><strong>Face Cropping</strong>: Extracting the face region with a 20% margin for context</li>
            <li><strong>Standardization</strong>: Resizing all faces to 224×224 pixels (standard input size for ResNet34)</li>
        </ol>

        <div class="note">
            <strong>Note:</strong> The face detection process filters out images where no face is detected, ensuring that only valid face images are used for training.
        </div>

        <h3>Example Code for Face Detection:</h3>
        <pre>
def process_image(img_path, output_path):
    """Detect face in image, crop, and save to output path"""
    try:
        # Load image using face_recognition
        image = face_recognition.load_image_file(img_path)
        
        # Find all face locations in the image
        face_locations = face_recognition.face_locations(image, model="hog")
        
        if not face_locations:
            print(f"No face found in {img_path}")
            return False
        
        # For simplicity, we'll use the first face found
        top, right, bottom, left = face_locations[0]
        
        # Add some margin to the face crop (20% of face size)
        height, width = bottom - top, right - left
        margin_h, margin_w = int(height * 0.2), int(width * 0.2)
        
        # Adjust boundaries with margins and ensure they're within image bounds
        img_h, img_w = image.shape[:2]
        top = max(0, top - margin_h)
        bottom = min(img_h, bottom + margin_h)
        left = max(0, left - margin_w)
        right = min(img_w, right + margin_w)
        
        # Crop the image to focus on the face
        face_image = image[top:bottom, left:right]
        pil_image = Image.fromarray(face_image)
        
        # Resize to a standard size (224x224 for resnet34)
        pil_image = pil_image.resize((224, 224), Image.Resampling.LANCZOS)
        
        # Save the processed image
        pil_image.save(output_path)
        return True
    except Exception as e:
        print(f"Error processing {img_path}: {e}")
        return False</pre>
    </div>

    <div class="container">
        <h2>Step 2: Creating a Siamese Network Dataset</h2>
        
        <p>
            For face recognition when we have only one image per person, a Siamese network approach is ideal. This requires creating pairs of images labeled as either "same" (same student with different augmentations) or "different" (different students).
        </p>

        <div class="siamese-network">
            <div class="diagram">
                <svg width="800" height="400" xmlns="http://www.w3.org/2000/svg">
                    <!-- Background -->
                    <rect x="0" y="0" width="800" height="400" fill="#ffffff" rx="10" ry="10"/>
                    
                    <!-- Same class pairs (left) -->
                    <rect x="50" y="50" width="300" height="130" fill="#e6f7ff" rx="5" ry="5" stroke="#0078d4" stroke-width="2"/>
                    <text x="200" y="30" font-family="Arial" font-size="16" text-anchor="middle" font-weight="bold">Positive Pairs (Same Student)</text>
                    
                    <rect x="70" y="70" width="80" height="80" fill="#cccccc" rx="5" ry="5"/>
                    <text x="110" y="110" font-family="Arial" font-size="12" text-anchor="middle">Original</text>
                    
                    <text x="170" y="110" font-family="Arial" font-size="20" text-anchor="middle" font-weight="bold">→</text>
                    
                    <rect x="200" y="70" width="80" height="80" fill="#cccccc" rx="5" ry="5"/>
                    <text x="240" y="110" font-family="Arial" font-size="12" text-anchor="middle">Aug 1</text>
                    
                    <text x="300" y="110" font-family="Arial" font-size="12" text-anchor="middle" font-weight="bold">"same"</text>
                    
                    <!-- Different class pairs (right) -->
                    <rect x="450" y="50" width="300" height="130" fill="#ffe6e6" rx="5" ry="5" stroke="#d83b01" stroke-width="2"/>
                    <text x="600" y="30" font-family="Arial" font-size="16" text-anchor="middle" font-weight="bold">Negative Pairs (Different Students)</text>
                    
                    <rect x="470" y="70" width="80" height="80" fill="#cccccc" rx="5" ry="5"/>
                    <text x="510" y="110" font-family="Arial" font-size="12" text-anchor="middle">Student A</text>
                    
                    <text x="570" y="110" font-family="Arial" font-size="20" text-anchor="middle" font-weight="bold">≠</text>
                    
                    <rect x="600" y="70" width="80" height="80" fill="#cccccc" rx="5" ry="5"/>
                    <text x="640" y="110" font-family="Arial" font-size="12" text-anchor="middle">Student B</text>
                    
                    <text x="700" y="110" font-family="Arial" font-size="12" text-anchor="middle" font-weight="bold">"different"</text>
                    
                    <!-- Siamese Network -->
                    <rect x="200" y="220" width="400" height="150" fill="#f0f7ff" rx="5" ry="5" stroke="#0078d4" stroke-width="2"/>
                    <text x="400" y="200" font-family="Arial" font-size="16" text-anchor="middle" font-weight="bold">Siamese Network Training</text>
                    
                    <rect x="220" y="240" width="120" height="60" fill="#0078d4" rx="5" ry="5"/>
                    <text x="280" y="270" font-family="Arial" font-size="12" text-anchor="middle" fill="white">CNN Encoder</text>
                    <text x="280" y="290" font-family="Arial" font-size="10" text-anchor="middle" fill="white">(ResNet34)</text>
                    
                    <rect x="460" y="240" width="120" height="60" fill="#0078d4" rx="5" ry="5"/>
                    <text x="520" y="270" font-family="Arial" font-size="12" text-anchor="middle" fill="white">CNN Encoder</text>
                    <text x="520" y="290" font-family="Arial" font-size="10" text-anchor="middle" fill="white">(Shared Weights)</text>
                    
                    <line x1="280" y1="300" x2="280" y2="330" stroke="#0078d4" stroke-width="2"/>
                    <line x1="520" y1="300" x2="520" y2="330" stroke="#0078d4" stroke-width="2"/>
                    <line x1="280" y1="330" x2="520" y2="330" stroke="#0078d4" stroke-width="2"/>
                    <line x1="400" y1="330" x2="400" y2="350" stroke="#0078d4" stroke-width="2"/>
                    
                    <rect x="350" y="350" width="100" height="30" fill="#0078d4" rx="5" ry="5"/>
                    <text x="400" y="370" font-family="Arial" font-size="12" text-anchor="middle" fill="white">Classifier</text>
                </svg>
            </div>
        </div>

        <h3>Creating Training Pairs</h3>

        <div class="grid">
            <div class="card">
                <h4>Positive Pairs (Same Class)</h4>
                <p>For each processed face image:</p>
                <ol>
                    <li>Apply multiple random augmentations using Albumentations</li>
                    <li>Each original image + augmented version creates a "same" pair</li>
                    <li>Transformations include rotation, flipping, brightness/contrast changes, and more</li>
                </ol>
            </div>

            <div class="card">
                <h4>Negative Pairs (Different Class)</h4>
                <p>For each negative example:</p>
                <ol>
                    <li>Randomly select two different student images</li>
                    <li>Resize both to 224×224 pixels</li>
                    <li>Label the pair as "different"</li>
                </ol>
            </div>
        </div>

        <div class="note">
            <strong>Note:</strong> The system creates a balanced dataset with approximately equal numbers of positive and negative pairs to prevent bias in training.
        </div>

        <h3>Data Augmentation Details:</h3>
        <pre>
# Define the augmentations using Albumentations
aug_albumentations = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.Rotate(limit=15, p=0.5),
    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
    A.GaussNoise(p=0.2), # Added some noise augmentation
    A.CoarseDropout(max_holes=8, max_height=8, max_width=8, fill_value=0, p=0.2), # Added dropout
    A.Resize(224, 224) # Albumentations applies resize as a transform
])</pre>
    </div>

    <div class="container">
        <h2>Step 3: Model Architecture and Training</h2>
        
        <p>
            The face recognition system uses a ResNet34 model pretrained on ImageNet as the backbone. This approach leverages transfer learning to achieve good results even with limited training data.
        </p>

        <h3>Model Architecture</h3>
        
        <p>The system uses ResNet34 in two ways:</p>
        <ol>
            <li><strong>During Training</strong>: As a classifier that learns to distinguish between "same" and "different" face pairs</li>
            <li><strong>For Inference</strong>: As a feature extractor that generates face embeddings (feature vectors)</li>
        </ol>

        <div class="diagram">
            <svg width="800" height="300" xmlns="http://www.w3.org/2000/svg">
                <!-- Background -->
                <rect x="0" y="0" width="800" height="300" fill="#ffffff" rx="10" ry="10"/>
                
                <!-- Model architecture -->
                <rect x="50" y="50" width="700" height="200" fill="#f8f9fa" rx="5" ry="5" stroke="#ddd" stroke-width="1"/>
                <text x="400" y="30" font-family="Arial" font-size="16" text-anchor="middle" font-weight="bold">ResNet34 Architecture for Face Recognition</text>
                
                <!-- Input -->
                <rect x="80" y="100" width="100" height="100" fill="#e6f7ff" rx="5" ry="5" stroke="#0078d4" stroke-width="2"/>
                <text x="130" y="150" font-family="Arial" font-size="12" text-anchor="middle">Input Image</text>
                <text x="130" y="170" font-family="Arial" font-size="10" text-anchor="middle">224 × 224 × 3</text>
                
                <!-- ResNet34 -->
                <rect x="220" y="80" width="250" height="140" fill="#0078d4" rx="5" ry="5"/>
                <text x="345" y="150" font-family="Arial" font-size="14" text-anchor="middle" fill="white">ResNet34 Backbone</text>
                <text x="345" y="170" font-family="Arial" font-size="10" text-anchor="middle" fill="white">(pretrained on ImageNet)</text>
                
                <!-- Features -->
                <rect x="510" y="100" width="100" height="40" fill="#ffcc99" rx="5" ry="5" stroke="#d83b01" stroke-width="2"/>
                <text x="560" y="125" font-family="Arial" font-size="12" text-anchor="middle">Feature Vector</text>
                
                <!-- Classification head -->
                <rect x="510" y="160" width="100" height="40" fill="#ccf0cc" rx="5" ry="5" stroke="#107c10" stroke-width="2"/>
                <text x="560" y="185" font-family="Arial" font-size="12" text-anchor="middle">Classification</text>
                
                <!-- Arrows -->
                <line x1="180" y1="150" x2="220" y2="150" stroke="#333" stroke-width="2"/>
                <line x1="470" y1="150" x2="490" y2="150" stroke="#333" stroke-width="2"/>
                <line x1="490" y1="150" x2="500" y2="150" stroke="#333" stroke-width="2"/>
                <line x1="490" y1="150" x2="490" y2="120" stroke="#333" stroke-width="2"/>
                <line x1="490" y1="120" x2="510" y2="120" stroke="#333" stroke-width="2"/>
                <line x1="490" y1="150" x2="490" y2="180" stroke="#333" stroke-width="2"/>
                <line x1="490" y1="180" x2="510" y2="180" stroke="#333" stroke-width="2"/>
                
                <!-- Labels -->
                <text x="200" y="140" font-family="Arial" font-size="10" text-anchor="middle">Images</text>
                <text x="490" y="110" font-family="Arial" font-size="10" text-anchor="middle">For face</text>
                <text x="490" y="120" font-family="Arial" font-size="10" text-anchor="middle">embeddings</text>
                <text x="490" y="170" font-family="Arial" font-size="10" text-anchor="middle">For</text>
                <text x="490" y="180" font-family="Arial" font-size="10" text-anchor="middle">training</text>
                
                <!-- Extraction point -->
                <circle cx="480" cy="150" r="5" fill="#d83b01"/>
            </svg>
        </div>

        <h3>Training Process</h3>

        <div class="grid">
            <div class="card">
                <h4>Initial Training</h4>
                <ul>
                    <li>Data split: 80% training, 20% validation</li>
                    <li>Learning rate: 1e-3</li>
                    <li>5 epochs with frozen weights (transfer learning)</li>
                    <li>Metrics: error_rate and accuracy</li>
                </ul>
            </div>

            <div class="card">
                <h4>Fine-tuning</h4>
                <ul>
                    <li>Unfreeze all layers</li>
                    <li>Learning rate: slice(1e-5, 1e-4) for discriminative learning rates</li>
                    <li>5 additional epochs</li>
                    <li>One-cycle learning policy</li>
                </ul>
            </div>
        </div>

        <h3>FastAI DataBlock API</h3>
        <p>The system uses FastAI's DataBlock API to handle data loading, augmentation, and batching:</p>

        <pre>
dblock = DataBlock(
    blocks=(ImageBlock, CategoryBlock),
    get_items=get_image_files,
    get_y=parent_label,
    splitter=RandomSplitter(valid_pct=0.2, seed=42),
    item_tfms=[Resize(224)],  # Ensure consistent size
    batch_tfms=[*aug_transforms(size=224), Normalize.from_stats(*imagenet_stats)]
)

dls = dblock.dataloaders(siamese_data_path, bs=32)</pre>

        <h3>Model Training Code</h3>
        <pre>
# Create and train the model
learn = cnn_learner(dls, resnet34, metrics=[error_rate, accuracy])

# Find optimal learning rate
learn.lr_find()

# Train the model
learn.fit_one_cycle(5, 1e-3)

# Fine-tune the model by unfreezing
learn.unfreeze()
learn.fit_one_cycle(5, slice(1e-5, 1e-4))</pre>
    </div>

    <div class="container">
        <h2>Step 4: Feature Extraction and Embeddings</h2>
        
        <p>
            After training, the model is used to extract face embeddings - high-dimensional vector representations of faces that capture distinctive features. These embeddings are then used for face comparison and recognition.
        </p>

        <h3>Feature Extraction Process</h3>
        <div class="workflow">
            <div class="workflow-step">
                <div class="step-number">1</div>
                <div class="step-content">
                    <strong>Input Preprocessing</strong>: Load and transform face image to model's required format
                </div>
            </div>
            <div class="workflow-step">
                <div class="step-number">2</div>
                <div class="step-content">
                    <strong>Forward Pass</strong>: Pass the image through the ResNet34 model, excluding the final classification layer
                </div>
            </div>
            <div class="workflow-step">
                <div class="step-number">3</div>
                <div class="step-content">
                    <strong>Feature Extraction</strong>: Capture the output of the penultimate layer (typically 512-dimensional vector)
                </div>
            </div>
            <div class="workflow-step">
                <div class="step-number">4</div>
                <div class="step-content">
                    <strong>Storage</strong>: Save the feature vectors for all known faces for later comparison
                </div>
            </div>
        </div>

        <h3>Feature Extraction Code</h3>
        <pre>
def extract_features(learn, img_path):
    """Extract features from the penultimate layer of the model"""
    import torch
    
    # Load and transform the image
    img = PILImage.create(img_path)
    
    # Create a test batch with a single image
    batch = learn.dls.test_dl([img])
    
    # Determine the device and move data appropriately
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Get the model's feature extractor (everything except the final layer)
    feature_extractor = learn.model[:-1].to(device)
    
    # Ensure the model is in eval mode
    feature_extractor.eval()
    
    # Extract features with gradient calculation disabled for efficiency
    with torch.no_grad():
        # Get the first batch from the dataloader
        batch_data = first(batch)
        
        # Handle different batch data structures
        if isinstance(batch_data, tuple):
            x = batch_data[0]
        else:
            x = batch_data
        
        # Move tensor to device
        x = x.to(device)
        
        # Get the embeddings/activations from the feature extractor
        activations = feature_extractor(x)
    
    # Convert to numpy array
    return activations[0].cpu().numpy()</pre>

        <div class="note">
            <strong>Important:</strong> The feature vectors are normalized before comparison to ensure that similarities are based on the angle between vectors (cosine similarity) rather than their magnitudes.
        </div>

        <h3>Precomputing Feature Vectors</h3>
        <p>For efficiency during inference, feature vectors for all processed faces are precomputed and stored:</p>

        <pre>
# Precompute and save feature vectors for all processed faces
feature_vectors = {}
processed_images = list(PROCESSED_PATH.glob('*.jpg'))

print(f"Computing feature vectors for {len(processed_images)} images...")
for i, img_path in enumerate(processed_images):
    if i % 50 == 0:
        print(f"Progress: {i}/{len(processed_images)}")
        
    features = extract_features(learn, img_path)
    feature_vectors[img_path.stem] = features

# Save feature vectors
np.save('./42amman_face_features.npy', feature_vectors)</pre>
    </div>

    <div class="container">
        <h2>Step 5: Face Recognition and Matching</h2>
        
        <p>
            The face recognition process compares a query face with all known faces to find potential matches. This is done by calculating the cosine similarity between feature vectors.
        </p>

        <h3>Recognition Process</h3>

        <div class="workflow">
            <div class="workflow-step">
                <div class="step-number">1</div>
                <div class="step-content">
                    <strong>Query Image Processing</strong>: Detect, crop, and align the face in the query image
                </div>
            </div>
            <div class="workflow-step">
                <div class="step-number">2</div>
                <div class="step-content">
                    <strong>Feature Extraction</strong>: Extract the feature vector from the query face
                </div>
            </div>
            <div class="workflow-step">
                <div class="step-number">3</div>
                <div class="step-content">
                    <strong>Similarity Calculation</strong>: Calculate cosine similarity with all known faces
                </div>
            </div>
            <div class="workflow-step">
                <div class="step-number">4</div>
                <div class="step-content">
                    <strong>Threshold Filtering</strong>: Filter matches based on a similarity threshold (e.g., 0.7)
                </div>
            </div>
            <div class="workflow-step">
                <div class="step-number">5</div>
                <div class="step-content">
                    <strong>Results Ranking</strong>: Rank potential matches by similarity score
                </div>
            </div>
        </div>

        <h3>Cosine Similarity</h3>
        <p>
            Cosine similarity measures the cosine of the angle between two vectors, providing a similarity score between -1 (completely different) and 1 (identical). In practice, face embedding similarities typically range from 0 to 1.
        </p>
        <div class="diagram">
            <svg width="400" height="250" xmlns="http://www.w3.org/2000/svg">
                <!-- Background -->
                <rect x="0" y="0" width="400" height="250" fill="#ffffff" rx="5" ry="5"/>
                
                <!-- Coordinate system -->
                <line x1="50" y1="200" x2="350" y2="200" stroke="#333" stroke-width="1"/>  <!-- X axis -->
                <line x1="50" y1="200" x2="50" y2="50" stroke="#333" stroke-width="1"/>    <!-- Y axis -->
                
                <!-- Vectors -->
                <line x1="50" y1="200" x2="200" y2="100" stroke="#0078d4" stroke-width="2"/>  <!-- Vector 1 -->
                <line x1="50" y1="200" x2="150" y2="150" stroke="#d83b01" stroke-width="2"/>  <!-- Vector 2 -->
                
                <!-- Angle -->
                <path d="M 80,190 A 30,30 0 0 0 85,170" fill="none" stroke="#333" stroke-width="1"/>
                <text x="90" y="180" font-family="Arial" font-size="12">θ</text>
                
                <!-- Labels -->
                <text x="200" y="100" font-family="Arial" font-size="12" text-anchor="start">Feature vector 1</text>
                <text x="150" y="160" font-family="Arial" font-size="12" text-anchor="start">Feature vector 2</text>
                <text x="200" y="220" font-family="Arial" font-size="12" text-anchor="middle">Cosine similarity = cos(θ)</text>
                
                <text x="200" y="50" font-family="Arial" font-size="14" font-weight="bold" text-anchor="middle">Cosine Similarity Between Face Vectors</text>
            </svg>
        </div>

        <h3>Recognition Function</h3>
        <pre>
def recognize_face(img_path, threshold=0.7):
    """Recognize a face in the given image"""
    # Process the query image to extract face
    query_face_path = Path('./query_face.jpg')
    face_found = process_image(img_path, query_face_path)
    
    if not face_found:
        return "No face detected in the query image", None
    
    # Extract features from the query face
    query_features = extract_features(learn, query_face_path)
    
    # Calculate cosine similarity with all known faces
    similarities = {}
    for name, features in feature_vectors.items():
        similarity = np.dot(query_features, features) / (
            np.linalg.norm(query_features) * np.linalg.norm(features))
        similarities[name] = float(similarity)
    
    # Sort by similarity score (highest first)
    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)
    
    # Filter by threshold
    matches = [match for match in sorted_similarities if match[1] >= threshold]
    
    if not matches:
        return "No matches found above the similarity threshold", sorted_similarities[:5]
    
    return matches, query_face_path</pre>

        <div class="note">
            <strong>Similarity Threshold:</strong> The threshold can be adjusted based on the desired balance between:
            <ul>
                <li>Higher threshold (e.g., 0.8+): More precision, fewer false positives, but might miss some true matches</li>
                <li>Lower threshold (e.g., 0.6-0.7): Higher recall, more potential matches, but might include some false positives</li>
            </ul>
        </div>
    </div>

    <div class="container">
        <h2>Interactive Interface</h2>
        
        <p>
            The face recognition system can be used through an interactive interface built with Gradio. This allows users to:
        </p>

        <div class="grid">
            <div class="card">
                <h3>Face Recognition</h3>
                <p>Upload a photo and find matching students from the database</p>
                <div class="image-placeholder">UI Screenshot Placeholder</div>
            </div>

            <div class="card">
                <h3>Face Verification</h3>
                <p>Compare two faces to check if they belong to the same person</p>
                <div class="image-placeholder">UI Screenshot Placeholder</div>
            </div>
        </div>

        <h3>Interface Setup</h3>
        <pre>
# Create the Gradio interface for face recognition
iface = gr.Interface(
    fn=recognize_face_gradio,
    inputs=[
        gr.Image(type="numpy", label="Upload a face image"),
        gr.Slider(minimum=0.5, maximum=0.95, value=0.7, step=0.05, label="Similarity Threshold")
    ],
    outputs=gr.Textbox(label="Recognition Results"),
    title="42 Amman Face Recognition",
    description="Upload a photo to check if it matches a student from 42 Amman."
)

# Launch the interface
iface.launch(share=True)</pre>
    </div>

    <div class="container">
        <h2>Performance Optimization and Deployment</h2>
        
        <p>
            The face recognition system can be optimized for better performance and deployed in various environments.
        </p>

        <h3>Model Export</h3>
        <p>
            The trained model can be exported for deployment in production environments:
        </p>
        <pre>
# Export model
learn.export('./42amman_face_model.pkl')

# Export feature vectors as a compressed file
np.savez_compressed('42amman_face_features.npz', feature_vectors=feature_vectors)</pre>

        <h3>Performance Considerations</h3>
        <div class="grid">
            <div class="card">
                <h4>Speed Optimization</h4>
                <ul>
                    <li>Use CPU or GPU inference based on available hardware</li>
                    <li>Batch processing for multiple faces when possible</li>
                    <li>Precompute and store feature vectors for known faces</li>
                </ul>
            </div>

            <div class="card">
                <h4>Accuracy Optimization</h4>
                <ul>
                    <li>Adjust face detection parameters for different scenarios</li>
                    <li>Fine-tune the similarity threshold based on use case</li>
                    <li>Regular model retraining with new images for improved performance</li>
                </ul>
            </div>
        </div>

        <div class="note">
            <strong>Best Practices:</strong>
            <ul>
                <li>Use clear, well-lit photos for best recognition results</li>
                <li>Ensure the face is clearly visible and not obscured</li>
                <li>For profile updates, periodically recompute feature vectors</li>
            </ul>
        </div>
    </div>

    <div class="container">
        <h2>Conclusion and Future Improvements</h2>
        
        <p>
            The 42 Amman Face Recognition System demonstrates how modern deep learning techniques can be applied to create an effective face recognition solution. The system uses a combination of face detection, feature extraction, and similarity matching to identify faces with high accuracy.
        </p>

        <h3>Key Achievements</h3>
        <ul>
            <li>Reliable face detection and alignment</li>
            <li>High-quality feature extraction using ResNet34</li>
            <li>Effective similarity-based matching</li>
            <li>User-friendly interface for face recognition and verification</li>
        </ul>

        <h3>Potential Improvements</h3>
        <div class="grid">
            <div class="card">
                <h4>Technical Enhancements</h4>
                <ul>
                    <li>Implement more advanced face alignment techniques</li>
                    <li>Explore newer model architectures (e.g., EfficientNet, Vision Transformer)</li>
                    <li>Use triplet loss or contrastive loss for direct embedding learning</li>
                </ul>
            </div>

            <div class="card">
                <h4>Feature Additions</h4>
                <ul>
                    <li>Support for video-based face recognition</li>
                    <li>Multi-face detection and recognition in group photos</li>
                    <li>Integration with access control systems</li>
                </ul>
            </div>
        </div>
    </div>

    <footer>
        <p>© 2025 42-Segfault Face Recognition Project | Created with FastAI and PyTorch</p>
    </footer>
</body>
</html>
